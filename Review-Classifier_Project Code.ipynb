{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from textblob.classifiers import word_tokenize, strip_punc\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_import(selected_class):\n",
    "    if selected_class == 'Bug':\n",
    "        #Bug Data\n",
    "        with open('Bug_tt.json') as infile_bug:\n",
    "            bugdata = json.load(infile_bug)\n",
    "        \n",
    "        bugdata = pd.DataFrame(bugdata)\n",
    "        return bugdata\n",
    "        # display(bugdata.head())\n",
    "    elif selected_class == \"Feature\":\n",
    "        #Feature Data\n",
    "        with open('Feature_tt.json') as infile_feature:\n",
    "            featuredata = json.load(infile_feature)\n",
    "\n",
    "        featuredata = pd.DataFrame(featuredata)\n",
    "        return featuredata\n",
    "        # display(featuredata.head())\n",
    "    elif selected_class == 'Rating':\n",
    "        #Rating Data\n",
    "        with open('Rating_tt.json') as infile_rating:\n",
    "            ratingdata = json.load(infile_rating)\n",
    "\n",
    "        ratingdata = pd.DataFrame(ratingdata)\n",
    "        return ratingdata\n",
    "        # display(ratingdata.head())\n",
    "    elif selected_class == 'UserExperience':\n",
    "        #User Experience Data\n",
    "        with open('UserExperience_tt.json') as infile_userexp:\n",
    "            userexpdata = json.load(infile_userexp)\n",
    "\n",
    "        userexpdata = pd.DataFrame(userexpdata)\n",
    "        return userexpdata\n",
    "        # display(traindata_userexp.head())\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_excel(\"Dataset3.xlsx\")\n",
    "#testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiScore = []\n",
    "for pos, neg in zip(testdata['sentiScore_pos'], testdata['sentiScore_neg']):\n",
    "    if abs(neg) > abs(pos):\n",
    "        sentiScore.append(neg)\n",
    "    else:\n",
    "        sentiScore.append(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "CUSTOM_STOPWORDS = ['i', 'me','up','my', 'myself', 'we', 'our', 'ours','ourselves', 'you', 'your', 'yours','yourself', \n",
    "                    'yourselves','he', 'him', 'his', 'himself', 'she', 'her', 'hers' ,'herself','it', 'its', 'itself', \n",
    "                    'they', 'them', 'their', 'theirs','themselves' ,'am', 'is', 'are','a', 'an', 'the', 'and','in','out', 'on','up','down', 's', 't']\n",
    "\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_comment = []\n",
    "stopwords_removal = []\n",
    "stopwords_removal_lemmatization = []\n",
    "stemmed = []\n",
    "stopwords_removal_nltk = []\n",
    "length = []\n",
    "\n",
    "for row in testdata['processed_text']:\n",
    "#     Length of words\n",
    "    length.append(len(str(row).split()))\n",
    "#     Tokenize\n",
    "    sentence_words = nltk.word_tokenize(row)\n",
    "#     print(sentence_words)\n",
    "    \n",
    "    lemmatized_comments = \" \"\n",
    "    stopword_removal = \" \"\n",
    "    stopword_lemmatize = \" \"\n",
    "    stemmed_word = \" \"\n",
    "    stopword_removal_nltk = \" \"\n",
    "    \n",
    "#     Lemmatize\n",
    "    for word in sentence_words:\n",
    "        lemmatize = wordnet_lemmatizer.lemmatize(word)\n",
    "        lemmatized_comments = str(lemmatized_comments)+\" \"+lemmatize\n",
    "#     print(lemmatized_comments)\n",
    " \n",
    "#     Stop Words Removal - Custom\n",
    "        sentence_lower = word.lower()\n",
    "        if not sentence_lower in CUSTOM_STOPWORDS:\n",
    "            stopword_removal = str(stopword_removal)+\" \"+sentence_lower\n",
    "#     print(stopword_removal)\n",
    "\n",
    "#     Stop Words Lemmatize\n",
    "            stop_lemmatize = wordnet_lemmatizer.lemmatize(sentence_lower)\n",
    "            stopword_lemmatize = str(stopword_lemmatize)+\" \"+stop_lemmatize\n",
    "#     print(stopword_lemmatize)\n",
    "\n",
    "#     Stemming\n",
    "        stemmer = s_stemmer.stem(word)\n",
    "        stemmed_word = str(stemmed_word)+\" \"+stemmer\n",
    "#     print(stemmed_word)      \n",
    "\n",
    "#     Stop Words Removal - NLTK\n",
    "        sentence_lower = word.lower()\n",
    "        if not sentence_lower in stop_words:\n",
    "            stopword_removal_nltk = str(stopword_removal_nltk)+\" \"+sentence_lower\n",
    "#     print(stopword_removal_nltk)\n",
    "\n",
    "#     Appending to lists\n",
    "    lemmatized_comment.append(lemmatized_comments)\n",
    "    stopwords_removal.append(stopword_removal)\n",
    "    stopwords_removal_lemmatization.append(stopword_lemmatize)\n",
    "    stemmed.append(stemmed_word)\n",
    "    stopwords_removal_nltk.append(stopword_removal_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding all the pre-processed columns to testdata\n",
    "testdata['lemmatized_comment'] = lemmatized_comment\n",
    "testdata['stopwords_removal'] = stopwords_removal\n",
    "testdata['stopwords_removal_lemmatization'] = stopwords_removal_lemmatization\n",
    "testdata['stemmed'] = stemmed\n",
    "testdata['stopwords_removal_nltk'] = stopwords_removal_nltk\n",
    "testdata['sentiScore'] = sentiScore\n",
    "testdata['length_words'] = length\n",
    "testdata.rename(columns={'score':'rating','processed_text':'comment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the test data to excel\n",
    "testdata.to_excel(\"Dataset3_processed.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data is now similar to the format of the train data to apply it to the classifer of the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the feature combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to generate combination of features**         \n",
    "- bow-comment\n",
    "- bigram-comment\n",
    "- bow-bigram-comment\n",
    "- bow-lemmatized_comment\n",
    "- bow-remove_stopwords\n",
    "- bow-stopwords_removal_lemmatization\n",
    "- bow-bigram-stopwords_removal_lemmatization\n",
    "- rating-comment\n",
    "- rating-comment-length\n",
    "- rating-comment-sentiment1-length\n",
    "- rating-comment-sentiment2-length\n",
    "- bow-rating-lemmatized_comment\n",
    "- bow-rating-comment-sentiment1\n",
    "- bigram-rating-comment-sentiment1\n",
    "- bigram-rating-stopwords_removal_lemmatization-sentiment2\n",
    "- bow-bigram-comment-sentiment1\n",
    "- bow-bigram-rating-lemmatized_comment\n",
    "- bow-bigram-remove_stopwords-rating-sentiment1\n",
    "- bow-rating-stopwords_removal_lemmatization-sentiment1\n",
    "- bow-rating-stopwords_removal_lemmatization-sentiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To create the configuration id (names of the above features)\n",
    "def get_key_for_classifier_config(cfg):\n",
    "    return \"-\".join([field for i,field in enumerate(cfg._fields) if not isinstance(cfg[i], str) and cfg[i]])\n",
    "\n",
    "\n",
    "# Defining the namedtuples\n",
    "classifier_technique_configurator = collections.namedtuple(\"classifier_technique_configurator\",\n",
    "                           [\n",
    "                            \"bow\",\n",
    "                            \"bigram\",\n",
    "                            \"remove_stopwords\",\n",
    "                            \"rating\",\n",
    "                            \"comment\",\n",
    "                            \"lemmatized_comment\",\n",
    "                            \"stopwords_removal_lemmatization\",                            \n",
    "                            \"sentiment1\",\n",
    "                            \"sentiment2\", \n",
    "                            \"length\"])\n",
    "\n",
    "\n",
    "# Setting the configuration parameters\n",
    "def get_classifier_technique_config(bow=False, bigram=False,comment = False, lemmatized_comment = False, remove_stopwords = False, stopwords_removal_lemmatization = False, rating = False, sentiment1 = False, sentiment2 = False, length = False):\n",
    "\n",
    "    classifier_technique_cfg = classifier_technique_configurator(\n",
    "        bow = bow,\n",
    "        bigram = bigram,\n",
    "        remove_stopwords = remove_stopwords,\n",
    "        rating = rating,\n",
    "        comment = comment,\n",
    "        lemmatized_comment = lemmatized_comment,\n",
    "        stopwords_removal_lemmatization = stopwords_removal_lemmatization,\n",
    "        sentiment1 = sentiment1,\n",
    "        sentiment2 = sentiment2,\n",
    "        length = length\n",
    "    )\n",
    "\n",
    "    return classifier_technique_cfg\n",
    "\n",
    "\n",
    "# Creating a dictionary with configuration id and its respective combination of features\n",
    "def get_combined_cfgs_journal_version():\n",
    "\n",
    "    toreturn = {}\n",
    "#    Document(Text) Matching\n",
    "\n",
    "#    Bow\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment=True, lemmatized_comment=False, remove_stopwords=False, stopwords_removal_lemmatization=False, rating=False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "    \n",
    "#    Bigram    \n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=True,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Bigram    \n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=True,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-lematize    \n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = False, lemmatized_comment = True, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Stopwords\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = False, lemmatized_comment = False, remove_stopwords = True,stopwords_removal_lemmatization = False, rating = False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Stopwords.Lemmatize\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = False, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = True, rating = False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Bigram-Stopwords.lematize    \n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=True,comment = False, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = True, rating = False, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Metadata\n",
    "\n",
    "#    Rating\n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=False,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "    \n",
    "#    Rating-Length    \n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=False,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = False, sentiment2 = False, length = True)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "    \n",
    "#    Rating-length-Sentiment1\n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=False,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = True, sentiment2 = False, length = True)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "    \n",
    "#    Rating-Length-Sentiment2\n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=False,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = False, sentiment2 = True, length = True)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Combined (Document(Text) & Metadata)\n",
    "\n",
    "#    Bow-Rating-Lematize\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = False, lemmatized_comment = True, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "    \n",
    "#    Bow-Rating-Sentiment1\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = True, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bigram-Rating-Sentiment1\n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=True,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = True, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bigram-Stowords.Lemtize-Rating-Sentiment2\n",
    "    cfg = get_classifier_technique_config(bow=False, bigram=True,comment = False, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = True, rating = True, sentiment1 = False, sentiment2 = True, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Bigram-Sentiment1\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=True,comment = True, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = False, sentiment1 = True, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Lematize-bigram-rating\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=True,comment = False, lemmatized_comment = True, remove_stopwords = False,stopwords_removal_lemmatization = False, rating = True, sentiment1 = False, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Stropwords-Bigram-Rating-Sentiment1\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=True,comment = False, lemmatized_comment = False, remove_stopwords = True,stopwords_removal_lemmatization = False, rating = True, sentiment1 = True, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Stopwords.Lematize-rating-sentiment1\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = False, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = True, rating = True, sentiment1 = True, sentiment2 = False, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "#    Bow-Stopwords.Lematize-Rating-Sentiment2\n",
    "    cfg = get_classifier_technique_config(bow=True, bigram=False,comment = False, lemmatized_comment = False, remove_stopwords = False,stopwords_removal_lemmatization = True, rating = True, sentiment1 = False, sentiment2 = True, length = False)\n",
    "    key = get_key_for_classifier_config(cfg)\n",
    "    toreturn[key] = cfg\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return toreturn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a set of all words from the selected columns** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_words_from_dataset(dataset):\n",
    "    \"\"\"Return a set of all words in a dataset.\n",
    "\n",
    "    :param dataset: A list of tuples of the form ``(words, label)`` where\n",
    "        ``words`` is either a string of a list of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Words may be either a string or a list of tokens. Return an iterator of tokens accordingly\n",
    "    \n",
    "    def tokenize(words):\n",
    "        if isinstance(words, str):\n",
    "            return word_tokenize(words, include_punc=False)\n",
    "        else:\n",
    "            return words\n",
    "        \n",
    "    if len(dataset[0]) == 2:\n",
    "        all_words = chain.from_iterable(tokenize(words.lower()) for words,_ in dataset)\n",
    "    else:\n",
    "        all_words = chain.from_iterable(tokenize(words.lower()) for words in dataset)\n",
    "\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing the below two operations for the combination of features to be passed for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigram_words(input_data):\n",
    "    return find_ngrams(input_data, 2)\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def extractor(document, word_features, feature_data):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cfg.comment:\n",
    "        score = feature_data['rating'][(feature_data['comment'] == document)].iloc[0]\n",
    "        sentiScore = feature_data['sentiScore'][(feature_data['comment'] == document)].iloc[0]\n",
    "        sentiScore_pos = feature_data['sentiScore_pos'][(feature_data['comment'] == document)].iloc[0]\n",
    "        sentiScore_neg = feature_data['sentiScore_neg'][(feature_data['comment'] == document)].iloc[0]\n",
    "        length = feature_data['length_words'][(feature_data['comment'] == document)].iloc[0]\n",
    "        #print(score)\n",
    "        #return (test_data['comment'] == document) == True\n",
    "\n",
    "\n",
    "\n",
    "    elif cfg.remove_stopwords:\n",
    "        score = feature_data['rating'][(feature_data['stopwords_removal_nltk'] == document)].iloc[0]\n",
    "        sentiScore = feature_data['sentiScore'][(feature_data['stopwords_removal_nltk'] == document)].iloc[0]\n",
    "        sentiScore_pos = feature_data['sentiScore_pos'][(feature_data['stopwords_removal_nltk'] == document)].iloc[0]\n",
    "        sentiScore_neg = feature_data['sentiScore_neg'][(feature_data['stopwords_removal_nltk'] == document)].iloc[0]\n",
    "        length = feature_data['length_words'][(feature_data['stopwords_removal_nltk'] == document)].iloc[0]\n",
    "\n",
    "        \n",
    "    elif cfg.stopwords_removal_lemmatization:\n",
    "        score = feature_data['rating'][(feature_data['stopwords_removal_lemmatization'] == document)].iloc[0]\n",
    "        sentiScore = feature_data['sentiScore'][(feature_data['stopwords_removal_lemmatization'] == document)].iloc[0]\n",
    "        sentiScore_pos = feature_data['sentiScore_pos'][(feature_data['stopwords_removal_lemmatization'] == document)].iloc[0]\n",
    "        sentiScore_neg = feature_data['sentiScore_neg'][(feature_data['stopwords_removal_lemmatization'] == document)].iloc[0]\n",
    "        length = feature_data['length_words'][(feature_data['stopwords_removal_lemmatization'] == document)].iloc[0]\n",
    "\n",
    "\n",
    "    elif cfg.lemmatized_comment:\n",
    "        score = feature_data['rating'][(feature_data['lemmatized_comment'] == document)].iloc[0]\n",
    "        sentiScore = feature_data['sentiScore'][(feature_data['lemmatized_comment'] == document)].iloc[0]\n",
    "        sentiScore_pos = feature_data['sentiScore_pos'][(feature_data['lemmatized_comment'] == document)].iloc[0]\n",
    "        sentiScore_neg = feature_data['sentiScore_neg'][(feature_data['lemmatized_comment'] == document)].iloc[0]\n",
    "        length = feature_data['length_words'][(feature_data['lemmatized_comment'] == document)].iloc[0]\n",
    "\n",
    "\n",
    "    \n",
    "    feats = {}\n",
    "    word_features = word_features\n",
    "    token_list = nltk.word_tokenize(document)\n",
    "    token_list = [word.lower() for word in token_list]\n",
    "    length_words = len(token_list)\n",
    "\n",
    "    if cfg.bow:\n",
    "        bow_features = dict(((u'contains({0})'.format(word), (word in token_list))\n",
    "                         for word in word_features))\n",
    "        #return bow_features\n",
    "        feats = feats.copy()\n",
    "        feats.update(bow_features)\n",
    "        \n",
    "    if cfg.bigram:\n",
    "        bigrams = extract_bigram_words(token_list)\n",
    "        bigram_features = dict()\n",
    "        for (w1,w2) in bigrams:\n",
    "            bigram_features[u'collocation({0},{1})'.format(w1,w2)] = True\n",
    "        #return feats\n",
    "        feats =  feats.copy()\n",
    "        feats.update(bigram_features)\n",
    "        \n",
    "    if cfg.length:\n",
    "        feats[\"length({0})\".format(length)] = True\n",
    "\n",
    "    if cfg.rating:\n",
    "        feats[\"rating({0})\".format(score)] = True\n",
    "    \n",
    "    if cfg.sentiment1:\n",
    "        feats[\"senti_Score({0})\".format(sentiScore)] = True\n",
    "    \n",
    "    if cfg.sentiment2:\n",
    "        feats[\"senti_Score_pos({0})\".format(sentiScore_pos)] = True\n",
    "        feats[\"senti_Score_neg({0})\".format(sentiScore_neg)] = True\n",
    "    \n",
    "    #print(feats)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_cfg = get_combined_cfgs_journal_version()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifying the selected class using NaiveBayes used by the author**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReviewClassifier(cfg_id, cfg, selected_class):\n",
    "    \n",
    "#     Import train and test data\n",
    "    train_data = data_import(selected_class)\n",
    "    test_data = pd.read_excel(\"Dataset3_processed.xlsx\")\n",
    "    cfg = cfg\n",
    "    \n",
    "    \n",
    "#     Selecting the column based on the configurations - train and test data\n",
    "    if cfg.comment:\n",
    "        train_data_features = []\n",
    "        for row,label in zip(train_data['comment'],train_data['label']):\n",
    "            train_data_features.append((row,label))\n",
    "        test_data_feature = []\n",
    "        for row in test_data['comment']:\n",
    "            test_data_feature.append(row)\n",
    "            \n",
    "    elif cfg.remove_stopwords:\n",
    "        train_data_features = []\n",
    "        for row,label in zip(train_data['stopwords_removal_nltk'],train_data['label']):\n",
    "            train_data_features.append((row,label))\n",
    "        test_data_feature = []\n",
    "        for row in test_data['stopwords_removal_nltk']:\n",
    "            test_data_feature.append(row)\n",
    "            \n",
    "    elif cfg.lemmatized_comment:\n",
    "        train_data_features = []\n",
    "        for row,label in zip(train_data['lemmatized_comment'],train_data['label']):\n",
    "            train_data_features.append((row,label))\n",
    "        test_data_feature = []\n",
    "        for row in test_data['lemmatized_comment']:\n",
    "            test_data_feature.append(row)\n",
    "             \n",
    "    elif cfg.stopwords_removal_lemmatization:\n",
    "        train_data_features = []\n",
    "        for row,label in zip(train_data['stopwords_removal_lemmatization'],train_data['label']):\n",
    "            train_data_features.append((row,label))\n",
    "        test_data_feature = []\n",
    "        for row in test_data['stopwords_removal_lemmatization']:\n",
    "            test_data_feature.append(row)\n",
    "            \n",
    "    random.shuffle(train_data_features)\n",
    "    \n",
    "#     Set of words from the selected column of train data\n",
    "    word_features = _get_words_from_dataset(train_data_features)\n",
    "    \n",
    "#     Dictionary that contains all the words that are present in word features and true for those present in each review that is used to train the model\n",
    "    train_features_labels = [(extractor(d, word_features, train_data), c) for d, c in train_data_features]\n",
    "    \n",
    "    \n",
    "#     Algorithm\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_features_labels)\n",
    "       \n",
    "#     Dictionary that contains all the words that are present in word features and true for those present in each review that is used to test the model\n",
    "    test_features_labels = [(extractor(d, word_features, test_data)) for d in test_data_feature]\n",
    "\n",
    "    testsets = collections.defaultdict(set)\n",
    "    predicted_labels = []\n",
    "    for i, (feats) in enumerate(test_features_labels):\n",
    "        observed = classifier.classify(feats)\n",
    "        predicted_labels.append(observed)\n",
    "        testsets[observed].add(i)\n",
    "    \n",
    "    test_data['Predicted_labels'] = predicted_labels \n",
    "    \n",
    "    filename = f'{selected_class}_{cfg_id}_predicted.csv'\n",
    "    \n",
    "    test_data.to_csv(filename)\n",
    "\n",
    "    \n",
    "    return testsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class Bug, Feature, UserExperience, Rating: UserExperience\n",
      "bow-comment\n",
      "bigram-comment\n",
      "bow-bigram-comment\n",
      "bow-lemmatized_comment\n",
      "bow-remove_stopwords\n",
      "bow-stopwords_removal_lemmatization\n",
      "bow-bigram-stopwords_removal_lemmatization\n",
      "rating-comment\n",
      "rating-comment-length\n",
      "rating-comment-sentiment1-length\n",
      "rating-comment-sentiment2-length\n",
      "bow-rating-lemmatized_comment\n",
      "bow-rating-comment-sentiment1\n",
      "bigram-rating-comment-sentiment1\n",
      "bigram-rating-stopwords_removal_lemmatization-sentiment2\n",
      "bow-bigram-comment-sentiment1\n",
      "bow-bigram-rating-lemmatized_comment\n",
      "bow-bigram-remove_stopwords-rating-sentiment1\n",
      "bow-rating-stopwords_removal_lemmatization-sentiment1\n",
      "bow-rating-stopwords_removal_lemmatization-sentiment2\n"
     ]
    }
   ],
   "source": [
    "# Choose the class file to classify using NaivesBayes algorithm\n",
    "selected_class = input(\"Select the class Bug, Feature, UserExperience, Rating: \" )\n",
    "testset_dict ={}\n",
    "for cfg_id,cfg in debug_cfg.items():\n",
    "    print(cfg_id)\n",
    "    testset =  ReviewClassifier(cfg_id,cfg,selected_class)\n",
    "    testset_dict[cfg_id] = testset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for:  UserExperience Reports\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification Techniques</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bow-comment</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.201183</td>\n",
       "      <td>0.306306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigram-comment</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.633540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bow-bigram-comment</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.349112</td>\n",
       "      <td>0.453846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow-lemmatized_comment</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.213018</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bow-remove_stopwords</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.100592</td>\n",
       "      <td>0.175258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bow-stopwords_removal_lemmatization</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bow-bigram-stopwords_removal_lemmatization</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.289941</td>\n",
       "      <td>0.406639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rating-comment</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.727811</td>\n",
       "      <td>0.560364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rating-comment-length</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.473373</td>\n",
       "      <td>0.521173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rating-comment-sentiment1-length</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.497041</td>\n",
       "      <td>0.518519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rating-comment-sentiment2-length</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>0.420118</td>\n",
       "      <td>0.453674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bow-rating-lemmatized_comment</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.195266</td>\n",
       "      <td>0.298643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bow-rating-comment-sentiment1</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.213018</td>\n",
       "      <td>0.325792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bigram-rating-comment-sentiment1</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.597753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bigram-rating-stopwords_removal_lemmatization-...</td>\n",
       "      <td>0.471861</td>\n",
       "      <td>0.644970</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bow-bigram-comment-sentiment1</td>\n",
       "      <td>0.651685</td>\n",
       "      <td>0.343195</td>\n",
       "      <td>0.449612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bow-bigram-rating-lemmatized_comment</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.360947</td>\n",
       "      <td>0.472868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bow-bigram-remove_stopwords-rating-sentiment1</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.284360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bow-rating-stopwords_removal_lemmatization-sen...</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bow-rating-stopwords_removal_lemmatization-sen...</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Classification Techniques  Precision    Recall  \\\n",
       "0                                         bow-comment   0.641509  0.201183   \n",
       "1                                      bigram-comment   0.487261  0.905325   \n",
       "2                                  bow-bigram-comment   0.648352  0.349112   \n",
       "3                              bow-lemmatized_comment   0.642857  0.213018   \n",
       "4                                bow-remove_stopwords   0.680000  0.100592   \n",
       "5                 bow-stopwords_removal_lemmatization   0.666667  0.153846   \n",
       "6          bow-bigram-stopwords_removal_lemmatization   0.680556  0.289941   \n",
       "7                                      rating-comment   0.455556  0.727811   \n",
       "8                               rating-comment-length   0.579710  0.473373   \n",
       "9                    rating-comment-sentiment1-length   0.541935  0.497041   \n",
       "10                   rating-comment-sentiment2-length   0.493056  0.420118   \n",
       "11                      bow-rating-lemmatized_comment   0.634615  0.195266   \n",
       "12                      bow-rating-comment-sentiment1   0.692308  0.213018   \n",
       "13                   bigram-rating-comment-sentiment1   0.481884  0.786982   \n",
       "14  bigram-rating-stopwords_removal_lemmatization-...   0.471861  0.644970   \n",
       "15                      bow-bigram-comment-sentiment1   0.651685  0.343195   \n",
       "16               bow-bigram-rating-lemmatized_comment   0.685393  0.360947   \n",
       "17      bow-bigram-remove_stopwords-rating-sentiment1   0.714286  0.177515   \n",
       "18  bow-rating-stopwords_removal_lemmatization-sen...   0.659091  0.171598   \n",
       "19  bow-rating-stopwords_removal_lemmatization-sen...   0.659091  0.171598   \n",
       "\n",
       "          F1  \n",
       "0   0.306306  \n",
       "1   0.633540  \n",
       "2   0.453846  \n",
       "3   0.320000  \n",
       "4   0.175258  \n",
       "5   0.250000  \n",
       "6   0.406639  \n",
       "7   0.560364  \n",
       "8   0.521173  \n",
       "9   0.518519  \n",
       "10  0.453674  \n",
       "11  0.298643  \n",
       "12  0.325792  \n",
       "13  0.597753  \n",
       "14  0.545000  \n",
       "15  0.449612  \n",
       "16  0.472868  \n",
       "17  0.284360  \n",
       "18  0.272300  \n",
       "19  0.272300  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_data = pd.DataFrame(columns=['Classification Techniques','Precision','Recall','F1'])\n",
    "cfg_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1score_list = []\n",
    "\n",
    "for cfg_id,cfg in debug_cfg.items():\n",
    "    filename = f'{selected_class}_{cfg_id}_predicted.csv'\n",
    "\n",
    "    refset = collections.defaultdict(set)\n",
    "\n",
    "    \n",
    "    ref_data = pd.read_excel(\"Dataset3.xlsx\")\n",
    "    if selected_class == \"Bug\":\n",
    "        for i, label in enumerate(ref_data['label_Bug']):\n",
    "            refset[label].add(i)\n",
    "    elif selected_class == \"Feature\":\n",
    "        for i, label in enumerate(ref_data['label_Feature']):\n",
    "            refset[label].add(i)\n",
    "            \n",
    "    elif selected_class == \"UserExperience\":\n",
    "        for i, label in enumerate(ref_data['label_UserExperience']):\n",
    "            refset[label].add(i)\n",
    "    elif selected_class == \"Rating\":\n",
    "        for i, label in enumerate(ref_data['label_Rating']):\n",
    "            refset[label].add(i)\n",
    "    #print(cfg_id)\n",
    "    #print(testset_dict[cfg_id][label])\n",
    "    p = nltk.precision(refset[label],testset_dict[cfg_id][label])\n",
    "#     print(\"Precision: \" , p)\n",
    "\n",
    "    r = nltk.recall(refset[label],testset_dict[cfg_id][label])\n",
    "#     print(\"Recall: \", r)\n",
    "\n",
    "    f = nltk.f_measure(refset[label],testset_dict[cfg_id][label])\n",
    "#     print(\"F1_Score: \", f)\n",
    "    \n",
    "    cfg_list.append(cfg_id)\n",
    "    precision_list.append(p)\n",
    "    recall_list.append(r)\n",
    "    f1score_list.append(f)\n",
    "\n",
    "metrics_data['Classification Techniques'] = cfg_list\n",
    "metrics_data['Precision'] = precision_list\n",
    "metrics_data['Recall'] = recall_list\n",
    "metrics_data['F1'] = f1score_list\n",
    "print(\"Metrics for: \", selected_class, \"Reports\")\n",
    "metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
